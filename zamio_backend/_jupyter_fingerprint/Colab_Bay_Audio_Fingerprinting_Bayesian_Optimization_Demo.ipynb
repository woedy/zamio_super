{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb7de757",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "b1853995",
      "metadata": {
        "id": "b1853995"
      },
      "source": [
        "# The New Bayesian Proo"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27d3b413",
      "metadata": {
        "id": "27d3b413"
      },
      "source": [
        "This notebook contains the updated code for fine-tuning an audio fingerprinting and matching system for Ghanaian music and radio clips using Bayesian optimization with Optuna.\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "- **Audio Files**: Place your full song WAV files in `./audio/songs` and corresponding 10-second clip WAV files in `./audio/clips`. Ensure clip filenames contain the corresponding song filename (e.g., `song1.wav` and `song1_clip1.wav`).\n",
        "- **Dependencies**: Install the required Python packages: `librosa`, `numba`, `xxhash`, `matplotlib`, `numpy`, `scipy`, `pandas`, and `optuna`.\n",
        "- **Environment**: Run in a Python environment with access to ffmpeg (for audio conversion/extraction if needed)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29e48cbe",
      "metadata": {
        "id": "29e48cbe"
      },
      "source": [
        "## Setup\n",
        "\n",
        "Install dependencies and create the audio directories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae2a9bbd",
      "metadata": {
        "id": "ae2a9bbd"
      },
      "outputs": [],
      "source": [
        "!pip install librosa numba xxhash matplotlib numpy scipy pandas optuna\n",
        "!mkdir -p ./audio/songs ./audio/clips"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae6f6f19",
      "metadata": {
        "id": "ae6f6f19"
      },
      "source": [
        "## Define Core Functions\n",
        "\n",
        "Define the necessary functions for audio loading, fingerprinting, and matching, including the corrected `get_2D_peaks_numba` and `generate_song_fingerprints` functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9537ff31",
      "metadata": {
        "id": "9537ff31"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import pandas as pd\n",
        "import optuna\n",
        "import numpy as np\n",
        "import librosa\n",
        "import matplotlib.pyplot as plt\n",
        "from numba import jit\n",
        "import xxhash\n",
        "from operator import itemgetter\n",
        "from collections import Counter\n",
        "import logging\n",
        "from typing import List, Tuple\n",
        "\n",
        "# Setup basic logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Configuration (will be overridden by optimized parameters)\n",
        "CONFIG = {\n",
        "    'DEFAULT_FS': 44100,\n",
        "    'DEFAULT_WINDOW_SIZE': 2048,\n",
        "    'DEFAULT_OVERLAP_RATIO': 0.5,\n",
        "    'DEFAULT_FAN_VALUE': 15,\n",
        "    'DEFAULT_AMP_MIN': -20,\n",
        "    'PEAK_NEIGHBORHOOD_SIZE': 10,\n",
        "    'MIN_HASH_TIME_DELTA': 0,\n",
        "    'MAX_HASH_TIME_DELTA': 500,\n",
        "    'FINGERPRINT_REDUCTION': 20,\n",
        "    'PEAK_SORT': True\n",
        "}\n",
        "\n",
        "@jit(nopython=True)\n",
        "def get_2D_peaks_numba(arr2D: np.ndarray, amp_min: float, peak_neighborhood_size: int) -> List[Tuple[int, int]]:\n",
        "    \"\"\"Optimized peak detection with numba.\"\"\"\n",
        "    peaks = []\n",
        "    rows, cols = arr2D.shape\n",
        "    neighborhood_size = peak_neighborhood_size // 2\n",
        "    for i in range(neighborhood_size, rows - neighborhood_size):\n",
        "        for j in range(neighborhood_size, cols - neighborhood_size):\n",
        "            if arr2D[i, j] > amp_min:\n",
        "                is_max = True\n",
        "                for di in range(-neighborhood_size, neighborhood_size + 1):\n",
        "                    for dj in range(-neighborhood_size, neighborhood_size + 1): # Corrected typo\n",
        "                        if di == 0 and dj == 0:\n",
        "                            continue\n",
        "                        if arr2D[i + di, j + dj] > arr2D[i, j]:\n",
        "                            is_max = False\n",
        "                            break\n",
        "                    if not is_max:\n",
        "                        break\n",
        "                if is_max:\n",
        "                    peaks.append((i, j))\n",
        "    return peaks\n",
        "\n",
        "def get_2D_peaks(arr2D: np.ndarray, plot: bool = False, amp_min: float = CONFIG['DEFAULT_AMP_MIN'],\n",
        "                 peak_neighborhood_size: int = CONFIG['PEAK_NEIGHBORHOOD_SIZE']) -> List[Tuple[int, int]]:\n",
        "    \"\"\"Extract peaks from spectrogram.\"\"\"\n",
        "    try:\n",
        "        peaks = get_2D_peaks_numba(arr2D, amp_min, peak_neighborhood_size)\n",
        "        # logger.info(f\"Detected {len(peaks)} peaks with amp_min={amp_min}\") # Removed for simplicity in this demo\n",
        "        if plot:\n",
        "            plt.figure(figsize=(10, 6))\n",
        "            plt.imshow(arr2D, origin='lower', aspect='auto', cmap='viridis')\n",
        "            if peaks:\n",
        "                freqs, times = zip(*peaks)\n",
        "                plt.scatter(times, freqs, c='r', s=10, label='Peaks')\n",
        "            plt.colorbar(label='Amplitude (dB)')\n",
        "            plt.xlabel('Time (frames)')\n",
        "            plt.ylabel('Frequency (bins)')\n",
        "            plt.title(f'Spectrogram with Detected Peaks (amp_min={amp_min})')\n",
        "            plt.legend()   \n",
        "            plt.show()\n",
        "        return peaks\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Peak detection failed: {e}\")\n",
        "        return []\n",
        "\n",
        "def generate_hashes(peaks: List[Tuple[int, int]], fan_value: int = CONFIG['DEFAULT_FAN_VALUE'],\n",
        "                    min_hash_time_delta: int = CONFIG['MIN_HASH_TIME_DELTA'],\n",
        "                    max_hash_time_delta: int = CONFIG['MAX_HASH_TIME_DELTA'],\n",
        "                    fingerprint_reduction: int = CONFIG['FINGERPRINT_REDUCTION'],\n",
        "                    peak_sort: bool = CONFIG['PEAK_SORT']) -> List[Tuple[str, int]]:\n",
        "    \"\"\"Generate hashes from peaks.\"\"\"\n",
        "    try:\n",
        "        if peak_sort:\n",
        "            peaks.sort(key=itemgetter(1))\n",
        "        hashes = []\n",
        "        valid_pairs = 0\n",
        "        for i in range(len(peaks)):\n",
        "            for j in range(1, fan_value):\n",
        "                if (i + j) < len(peaks):\n",
        "                    freq1 = peaks[i][0]\n",
        "                    freq2 = peaks[i + j][0]\n",
        "                    t1 = peaks[i][1]\n",
        "                    t2 = peaks[i + j][1]\n",
        "                    t_delta = t2 - t1\n",
        "                    if min_hash_time_delta <= t_delta <= max_hash_time_delta:\n",
        "                        valid_pairs += 1\n",
        "                        h = xxhash.xxh64(f\"{freq1}|{freq2}|{t_delta}\".encode('utf-8'))\n",
        "                        hash_str = h.hexdigest()[:fingerprint_reduction]\n",
        "                        hashes.append((hash_str, t1))\n",
        "        # logger.info(f\"Generated {valid_pairs} valid peak pairs for hashing\") # Removed for simplicity in this demo\n",
        "        return hashes\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Hash generation failed: {e}\")\n",
        "        return []\n",
        "\n",
        "def fingerprint(channel_samples: np.ndarray, Fs: int = CONFIG['DEFAULT_FS'],\n",
        "                wsize: int = CONFIG['DEFAULT_WINDOW_SIZE'], wratio: float = CONFIG['DEFAULT_OVERLAP_RATIO'],\n",
        "                fan_value: int = CONFIG['DEFAULT_FAN_VALUE'], amp_min: float = CONFIG['DEFAULT_AMP_MIN'],\n",
        "                peak_neighborhood_size: int = CONFIG['PEAK_NEIGHBORHOOD_SIZE'],\n",
        "                min_hash_time_delta: int = CONFIG['MIN_HASH_TIME_DELTA'],\n",
        "                max_hash_time_delta: int = CONFIG['MAX_HASH_TIME_DELTA'],\n",
        "                fingerprint_reduction: int = CONFIG['FINGERPRINT_REDUCTION'],\n",
        "                peak_sort: bool = CONFIG['PEAK_SORT']) -> List[Tuple[str, int]]:\n",
        "    \"\"\"Generate fingerprints from audio samples.\"\"\"\n",
        "    try:\n",
        "        samples = channel_samples.astype(np.float32) / 32768.0\n",
        "        hop_length = int(wsize * (1 - wratio))\n",
        "        S = librosa.stft(samples, n_fft=wsize, hop_length=hop_length, window='hann')\n",
        "        arr2D = librosa.amplitude_to_db(np.abs(S), ref=np.max)\n",
        "        # logger.info(f\"Spectrogram min: {arr2D.min():.2f}, max: {arr2D.max():.2f}\") # Removed for simplicity\n",
        "        local_maxima = get_2D_peaks(arr2D, amp_min=amp_min, peak_neighborhood_size=peak_neighborhood_size)\n",
        "        hashes = generate_hashes(local_maxima, fan_value=fan_value,\n",
        "                                min_hash_time_delta=min_hash_time_delta,\n",
        "                                max_hash_time_delta=max_hash_time_delta,\n",
        "                                fingerprint_reduction=fingerprint_reduction,\n",
        "                                peak_sort=peak_sort)\n",
        "        # logger.info(f\"Generated {len(hashes)} fingerprints for {len(samples)/Fs:.2f}s audio\") # Removed for simplicity\n",
        "        return hashes\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Fingerprinting failed: {e}\")\n",
        "        return []\n",
        "\n",
        "def load_audio(file_path: str) -> Tuple[np.ndarray, int]:\n",
        "    \"\"\"Load audio file and convert to int16.\"\"\"\n",
        "    try:\n",
        "        if not os.path.exists(file_path):\n",
        "            logger.error(f\"File not found: {file_path}\")\n",
        "            return np.array([]), 0\n",
        "        samples, sr = librosa.load(file_path, sr=CONFIG['DEFAULT_FS'], mono=True)\n",
        "        samples = (samples * 32768).astype(np.int16)\n",
        "        # logger.info(f\"Loaded {file_path}: {len(samples)} samples, {sr} Hz, max amplitude: {np.max(np.abs(samples))}\") # Removed for simplicity\n",
        "        return samples, sr\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Failed to load {file_path}: {e}\")\n",
        "        return np.array([]), 0\n",
        "\n",
        "# Simulate Django Song model (simplified)\n",
        "class Song:\n",
        "    def __init__(self, id, title):\n",
        "        self.id = id\n",
        "        self.title = title\n",
        "\n",
        "def generate_song_fingerprints(samples, sr, song_id, **params):\n",
        "    \"\"\"Generate fingerprints for a song using provided parameters.\"\"\"\n",
        "    if len(samples) == 0:\n",
        "        logger.error(\"No samples provided for fingerprinting\")\n",
        "        return []\n",
        "\n",
        "    # Extract only fingerprinting parameters from the params dictionary\n",
        "    fingerprint_params = {\n",
        "        key: params[key] for key in ['wsize', 'wratio', 'fan_value', 'amp_min',\n",
        "                                     'peak_neighborhood_size', 'min_hash_time_delta',\n",
        "                                     'max_hash_time_delta', 'fingerprint_reduction', 'peak_sort']\n",
        "        if key in params\n",
        "    }\n",
        "\n",
        "    fingerprints = fingerprint(samples, Fs=sr, **fingerprint_params)\n",
        "    db_fingerprints = [(song_id, h, o) for h, o in fingerprints]\n",
        "    return db_fingerprints\n",
        "\n",
        "def match_clip(clip_samples, clip_sr, db_fingerprints, **params):\n",
        "    \"\"\"Match clip fingerprints against database using provided parameters.\"\"\"\n",
        "    if len(clip_samples) == 0:\n",
        "        return {\"match\": False, \"reason\": \"No samples in clip\"}\n",
        "\n",
        "    # Extract only fingerprinting parameters for generating clip fingerprints\n",
        "    fingerprint_params = {\n",
        "        key: params[key] for key in ['wsize', 'wratio', 'fan_value', 'amp_min',\n",
        "                                     'peak_neighborhood_size', 'min_hash_time_delta',\n",
        "                                     'max_hash_time_delta', 'fingerprint_reduction', 'peak_sort']\n",
        "        if key in params\n",
        "    }\n",
        "\n",
        "    # Generate clip fingerprints\n",
        "    clip_fingerprints = fingerprint(clip_samples, Fs=clip_sr, **fingerprint_params)\n",
        "\n",
        "    if not clip_fingerprints:\n",
        "        return {\"match\": False, \"reason\": \"No fingerprints extracted\", \"hashes_matched\": 0,\n",
        "                \"input_confidence\": 0.0, \"db_confidence\": 0.0}\n",
        "\n",
        "    # Match fingerprints\n",
        "    query_hashes = [h for h, _ in clip_fingerprints]\n",
        "    db_hashes = {(h, o, song_id) for song_id, h, o in db_fingerprints if h in query_hashes}\n",
        "    if not db_hashes:\n",
        "        return {\"match\": False, \"reason\": \"No matching hashes found\", \"hashes_matched\": 0,\n",
        "                \"input_confidence\": 0.0, \"db_confidence\": 0.0}\n",
        "\n",
        "    match_map = Counter()\n",
        "    for h, query_offset in clip_fingerprints:\n",
        "        for db_hash, db_offset, song_id in db_hashes:\n",
        "            if h == db_hash:\n",
        "                offset_diff = db_offset - query_offset\n",
        "                match_map[(song_id, offset_diff)] += 1\n",
        "\n",
        "    if not match_map:\n",
        "        return {\"match\": False, \"reason\": \"No offset alignment found\", \"hashes_matched\": 0,\n",
        "                \"input_confidence\": 0.0, \"db_confidence\": 0.0}\n",
        "\n",
        "    (song_id, offset_diff), match_count = match_map.most_common(1)[0]\n",
        "    total_query_hashes = len(query_hashes)\n",
        "    total_db_hashes = sum(1 for _, _, sid in db_fingerprints if sid == song_id)\n",
        "    input_confidence = (match_count / total_query_hashes) * 100\n",
        "    db_confidence = (match_count / total_db_hashes) * 100 if total_db_hashes else 0\n",
        "\n",
        "    min_match_count = params.get('min_match_count', 10)\n",
        "    min_input_conf = params.get('min_input_conf', 10.0)\n",
        "    min_db_conf = params.get('min_db_conf', 2.0)\n",
        "\n",
        "\n",
        "    if match_count < min_match_count or input_confidence < min_input_conf or db_confidence < min_db_conf:\n",
        "        return {\n",
        "            \"match\": False,\n",
        "            \"reason\": \"Low confidence match\",\n",
        "            \"hashes_matched\": match_count,\n",
        "            \"input_confidence\": input_confidence,\n",
        "            \"db_confidence\": db_confidence\n",
        "        }\n",
        "\n",
        "    return {\n",
        "        \"match\": True,\n",
        "        \"song_id\": song_id,\n",
        "        \"offset\": offset_diff,\n",
        "        \"hashes_matched\": match_count,\n",
        "        \"input_confidence\": input_confidence,\n",
        "        \"db_confidence\": db_confidence\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "630e317e",
      "metadata": {
        "id": "630e317e"
      },
      "source": [
        "## Load Dataset\n",
        "\n",
        "Load the audio files from the specified directories and create song-clip pairs for evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7efa948e",
      "metadata": {
        "id": "7efa948e"
      },
      "outputs": [],
      "source": [
        "# Define the directories for full songs and clips\n",
        "SONG_DIR = './audio/songs'\n",
        "CLIP_DIR = './audio/clips'\n",
        "\n",
        "# IMPORTANT: Please ensure your full song WAV files are in the ./audio/songs directory\n",
        "# and your 10-second clip WAV files are in the ./audio/clips directory before running this cell.\n",
        "# Ensure clip filenames contain the corresponding song filename (e.g., song1.wav and song1_clip1.wav).\n",
        "\n",
        "\n",
        "def load_audio_files(directory: str) -> dict:\n",
        "    \"\"\"Recursively load audio files from a directory.\"\"\"\n",
        "    audio_data = {}\n",
        "    for file_path in glob.glob(os.path.join(directory, '**/*.wav'), recursive=True):\n",
        "        samples, sr = load_audio(file_path)\n",
        "        if len(samples) > 0:\n",
        "            audio_data[os.path.basename(file_path)] = (samples, sr)\n",
        "    return audio_data\n",
        "\n",
        "# Load all songs and clips\n",
        "all_songs = load_audio_files(SONG_DIR)\n",
        "all_clips = load_audio_files(CLIP_DIR)\n",
        "\n",
        "print(f\"Loaded {len(all_songs)} songs from {SONG_DIR}\")\n",
        "print(f\"Loaded {len(all_clips)} clips from {CLIP_DIR}\")\n",
        "\n",
        "# Create song-clip pairs. Assuming clip filenames contain the corresponding song filename.\n",
        "# Example: song1.wav and song1_clip1.wav, song1_clip2.wav\n",
        "song_clip_pairs = []\n",
        "song_id_counter = 0\n",
        "song_mapping = {} # Map song filename to song_id\n",
        "\n",
        "for song_filename, (song_samples, song_sr) in all_songs.items():\n",
        "    song_id_counter += 1\n",
        "    current_song_id = song_id_counter\n",
        "    song_mapping[song_filename] = current_song_id\n",
        "\n",
        "    corresponding_clips = [(clip_filename, (clip_samples, clip_sr))\n",
        "                           for clip_filename, (clip_samples, clip_sr) in all_clips.items()\n",
        "                           if song_filename.split('.')[0] in clip_filename] # Simple filename matching\n",
        "\n",
        "    if corresponding_clips:\n",
        "        for clip_filename, (clip_samples, clip_sr) in corresponding_clips:\n",
        "            song_clip_pairs.append({\n",
        "                'song_id': current_song_id,\n",
        "                'song_filename': song_filename,\n",
        "                'song_samples': song_samples,\n",
        "                'song_sr': song_sr,\n",
        "                'clip_filename': clip_filename,\n",
        "                'clip_samples': clip_samples,\n",
        "                'clip_sr': clip_sr\n",
        "            })\n",
        "    else:\n",
        "         logger.warning(f\"No corresponding clips found for song: {song_filename}\")\n",
        "\n",
        "print(f\"Created {len(song_clip_pairs)} song-clip pairs for evaluation.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f2cbbeb",
      "metadata": {
        "id": "7f2cbbeb"
      },
      "source": [
        "## Define Optimization Objective and Parameter Ranges\n",
        "\n",
        "Define the objective function for Optuna and the search space for the parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c7fee5c",
      "metadata": {
        "id": "0c7fee5c"
      },
      "outputs": [],
      "source": [
        "# Define parameter ranges tailored for Ghanaian music (Adjusted ranges)\n",
        "param_ranges = {\n",
        "    'amp_min': (-35, -10),\n",
        "    'peak_neighborhood_size': (5, 12),\n",
        "    'fan_value': (10, 20),\n",
        "    'min_match_count': (5, 15),\n",
        "    'min_input_conf': (5.0, 15.0),\n",
        "    'min_db_conf': (1.0, 3.0),\n",
        "    'wsize': [1024, 2048, 4096],\n",
        "    'wratio': (0.5, 0.85),\n",
        "    'min_hash_time_delta': (0, 10),\n",
        "    'max_hash_time_delta': (200, 500),\n",
        "    'fingerprint_reduction': [16, 20, 24],\n",
        "    'peak_sort': [True, False]\n",
        "}\n",
        "\n",
        "# Update evaluate_params to process the dataset\n",
        "def evaluate_params_on_dataset(dataset: List[dict], params: dict):\n",
        "    \"\"\"Evaluate parameters across the entire dataset.\"\"\"\n",
        "    total_score = 0\n",
        "    trial_results = []\n",
        "\n",
        "    if not dataset:\n",
        "        return 0, [] # Return 0 score and empty results if dataset is empty\n",
        "\n",
        "    for item in dataset:\n",
        "        song_id = item['song_id']\n",
        "        song_samples = item['song_samples']\n",
        "        song_sr = item['song_sr']\n",
        "        clip_samples = item['clip_samples']\n",
        "        clip_sr = item['clip_sr']\n",
        "\n",
        "        # Generate song fingerprints\n",
        "        song_fingerprint_params = {\n",
        "            key: params[key] for key in ['wsize', 'wratio', 'fan_value', 'amp_min',\n",
        "                                         'peak_neighborhood_size', 'min_hash_time_delta',\n",
        "                                         'max_hash_time_delta', 'fingerprint_reduction', 'peak_sort']\n",
        "            if key in params\n",
        "        }\n",
        "        song_fingerprints = generate_song_fingerprints(song_samples, song_sr, song_id, **song_fingerprint_params)\n",
        "\n",
        "        # Generate clip fingerprints and match\n",
        "        clip_fingerprint_params = {\n",
        "            key: params[key] for key in ['wsize', 'wratio', 'fan_value', 'amp_min',\n",
        "                                         'peak_neighborhood_size', 'min_hash_time_delta',\n",
        "                                         'max_hash_time_delta', 'fingerprint_reduction', 'peak_sort']\n",
        "            if key in params\n",
        "        }\n",
        "        result = match_clip(clip_samples, clip_sr, song_fingerprints, **params)\n",
        "\n",
        "        # Compute metrics (re-using the scoring logic from the original evaluate_params)\n",
        "        # Need to pass parameters to fingerprint function when calculating clip_fingerprints_len\n",
        "        clip_fingerprints_len = len([h for h, _ in fingerprint(clip_samples, Fs=clip_sr, **clip_fingerprint_params)])\n",
        "        song_fingerprints_len = len(song_fingerprints)\n",
        "\n",
        "        max_clip_fingerprints = clip_fingerprints_len if clip_fingerprints_len > 0 else 1\n",
        "        max_song_fingerprints = song_fingerprints_len if song_fingerprints_len > 0 else 1\n",
        "\n",
        "        score = (\n",
        "            (clip_fingerprints_len / max_clip_fingerprints) * 0.3 +\n",
        "            (song_fingerprints_len / max_song_fingerprints) * 0.3 +\n",
        "            result.get('input_confidence', 0.0) * 0.2 +\n",
        "            result.get('db_confidence', 0.0) * 0.2\n",
        "        ) if result['match'] else 0\n",
        "\n",
        "\n",
        "        trial_results.append({\n",
        "            'song_id': song_id,\n",
        "            'song_filename': item['song_filename'],\n",
        "            'clip_filename': item['clip_filename'],\n",
        "            'clip_fingerprints': clip_fingerprints_len,\n",
        "            'song_fingerprints': song_fingerprints_len,\n",
        "            'match': result['match'],\n",
        "            'hashes_matched': result.get('hashes_matched', 0),\n",
        "            'input_confidence': result.get('input_confidence', 0.0),\n",
        "            'db_confidence': result.get('db_confidence', 0.0),\n",
        "            'reason': result.get('reason', ''),\n",
        "            'score': score,\n",
        "            **params # Include parameters in the results for analysis\n",
        "        })\n",
        "        total_score += score\n",
        "\n",
        "    # Return average score or a combined metric\n",
        "    average_score = total_score / len(dataset) if dataset else 0\n",
        "    return average_score, trial_results\n",
        "\n",
        "\n",
        "# Define objective function for optuna using the dataset\n",
        "def objective_dataset(trial):\n",
        "    params = {\n",
        "        'amp_min': trial.suggest_float('amp_min', param_ranges['amp_min'][0], param_ranges['amp_min'][1]),\n",
        "        'peak_neighborhood_size': trial.suggest_int('peak_neighborhood_size', param_ranges['peak_neighborhood_size'][0], param_ranges['peak_neighborhood_size'][1]),\n",
        "        'fan_value': trial.suggest_int('fan_value', param_ranges['fan_value'][0], param_ranges['fan_value'][1]),\n",
        "        'min_match_count': trial.suggest_int('min_match_count', param_ranges['min_match_count'][0], param_ranges['min_match_count'][1]),\n",
        "        'min_input_conf': trial.suggest_float('min_input_conf', param_ranges['min_input_conf'][0], param_ranges['min_input_conf'][1]),\n",
        "        'min_db_conf': trial.suggest_float('min_db_conf', param_ranges['min_db_conf'][0], param_ranges['min_db_conf'][1]),\n",
        "        'wsize': trial.suggest_categorical('wsize', param_ranges['wsize']),\n",
        "        'wratio': trial.suggest_float('wratio', param_ranges['wratio'][0], param_ranges['wratio'][1]),\n",
        "        'min_hash_time_delta': trial.suggest_int('min_hash_time_delta', param_ranges['min_hash_time_delta'][0], param_ranges['min_hash_time_delta'][1]),\n",
        "        'max_hash_time_delta': trial.suggest_int('max_hash_time_delta', param_ranges['max_hash_time_delta'][0], param_ranges['max_hash_time_delta'][1]),\n",
        "        'fingerprint_reduction': trial.suggest_categorical('fingerprint_reduction', param_ranges['fingerprint_reduction']),\n",
        "        'peak_sort': trial.suggest_categorical('peak_sort', param_ranges['peak_sort'])\n",
        "    }\n",
        "    average_score, _ = evaluate_params_on_dataset(song_clip_pairs, params)\n",
        "    print(f\"Trial {trial.number}: average_score={average_score:.4f}, params={params}\")\n",
        "    return average_score"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c171100",
      "metadata": {
        "id": "3c171100"
      },
      "source": [
        "## Run Bayesian Optimization\n",
        "\n",
        "Execute the Bayesian optimization process. This can be computationally intensive depending on the dataset size and number of trials."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bae76193",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "bae76193"
      },
      "outputs": [],
      "source": [
        "# Run Bayesian optimization on the dataset\n",
        "if song_clip_pairs: # Only run optimization if there are pairs\n",
        "    print(f\"Starting Bayesian Optimization with {len(song_clip_pairs)} song-clip pairs and {param_ranges} parameter ranges.\")\n",
        "    study_dataset = optuna.create_study(direction='maximize')\n",
        "    study_dataset.optimize(objective_dataset, n_trials=500) # Increased n_trials to 500\n",
        "\n",
        "    # Collect results for the best trial on the dataset\n",
        "    best_params_dataset = study_dataset.best_trial.params\n",
        "    average_score, all_trial_results = evaluate_params_on_dataset(song_clip_pairs, best_params_dataset)\n",
        "\n",
        "    # Create DataFrame from all trial results for detailed analysis\n",
        "    df_all_trials = pd.DataFrame(all_trial_results)\n",
        "\n",
        "    # Print the best parameters and average score\n",
        "    print(\"\\nBest Configuration (Dataset Evaluation):\")\n",
        "    print(best_params_dataset)\n",
        "    print(f\"Average Score with Best Configuration: {average_score:.4f}\")\n",
        "\n",
        "    # Display results table for the best trial's evaluation\n",
        "    print(\"\\nDetailed Results for Best Configuration on Dataset:\")\n",
        "    display(df_all_trials[['song_filename', 'clip_filename', 'match', 'hashes_matched', 'input_confidence', 'db_confidence', 'score', 'reason']])\n",
        "\n",
        "else:\n",
        "    print(\"\\nNo song-clip pairs found. Please add audio files to ./audio/songs and ./audio/clips.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "57955102",
      "metadata": {
        "id": "57955102"
      },
      "source": [
        "## Analyze Optimization Results\n",
        "\n",
        "Analyze the results of the optimization study, including the best performing parameters and visualizations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0a6f4f3",
      "metadata": {
        "id": "d0a6f4f3"
      },
      "outputs": [],
      "source": [
        "import optuna.visualization as optuna_vis\n",
        "\n",
        "# Access the best trial's parameters and value\n",
        "if 'study_dataset' in locals() and study_dataset.trials:\n",
        "    print(\"\\nBest trial:\")\n",
        "    print(f\"  Value (Score): {study_dataset.best_trial.value:.4f}\")\n",
        "    print(\"  Params:\")\n",
        "    for key, value in study_dataset.best_trial.params.items():\n",
        "        print(f\"    {key}: {value}\")\n",
        "\n",
        "    # Generate and display visualization plots\n",
        "    print(\"\\nGenerating optimization history plot...\")\n",
        "    fig_history = optuna_vis.plot_optimization_history(study_dataset)\n",
        "    fig_history.show()\n",
        "\n",
        "    print(\"\\nGenerating parallel coordinate plot...\")\n",
        "    fig_parallel = optuna_vis.plot_parallel_coordinate(study_dataset)\n",
        "    fig_parallel.show()\n",
        "\n",
        "    try:\n",
        "        print(\"\\nGenerating parameter importance plot...\")\n",
        "        fig_importance = optuna_vis.plot_param_importances(study_dataset)\n",
        "        fig_importance.show()\n",
        "    except RuntimeError as e:\n",
        "        print(f\"Could not generate parameter importance plot: {e}. This can happen if there is zero variance in the trial results.\")\n",
        "\n",
        "else:\n",
        "    print(\"Optimization study was not run or contains no trials. Please ensure audio files are in place and the optimization step completed successfully.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
